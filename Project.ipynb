{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "#note: this requires the starter code for the assignments!\n",
    "from common.plotting import plot_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building dataset:\n",
    "\n",
    "[Customizable: trainset size]\n",
    "\n",
    "Initializing all data picking train/validation sets ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "#trainset [0,50k]\n",
    "trainset=40000\n",
    "cifar_train = CIFAR10((\"train\",), subset=slice(None,trainset))\n",
    "\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "cifar_train_stream = DataStream.default_stream(\n",
    "    cifar_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar_train.num_examples, 25))\n",
    "cifar_validation = CIFAR10((\"train\",), subset=slice(trainset, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "cifar_validation_stream = DataStream.default_stream(\n",
    "    cifar_validation, iteration_scheme=SequentialScheme(cifar_validation.num_examples, 25))\n",
    "cifar_test = CIFAR10((\"test\",))\n",
    "cifar_test_stream = DataStream.default_stream(\n",
    "    cifar_test, iteration_scheme=SequentialScheme(cifar_test.num_examples, 25))\n",
    "\n",
    "print \"The streams return batches containing %s\" % (cifar_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "srng = theano.tensor.shared_randomstreams.RandomStreams(rng.randint(999999))\n",
    "class Layer_ReLU():\n",
    "    \"Creates ReLU layer\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def apply(self,input_tensor,name):\n",
    "        return [None,None,theano.tensor.maximum(0.0,input_tensor)]\n",
    "\n",
    "class Layer_Conv():\n",
    "    def __init__(self,num_filters,filter_size,init_weight=None,padding=0):\n",
    "            self.num_filters=num_filters\n",
    "            self.filter_size=filter_size\n",
    "            self.padding=padding\n",
    "            if init_weight is not None:\n",
    "                self.init_weight=init_weight\n",
    "            else:\n",
    "                 self.init_weight=0.01\n",
    "    \n",
    "    def apply(self,input_tensor,name):\n",
    "        \n",
    "        CW = theano.shared(np.zeros((self.num_filters,input_tensor.tag.test_value.shape[1],self.filter_size,self.filter_size),\n",
    "                                    dtype='float32'),name='CW'+name)\n",
    "        CW.tag.initializer= IsotropicGaussian(self.init_weight)\n",
    "        CB = theano.shared(np.zeros((self.num_filters), dtype='float32'),\n",
    "                    name='CB'+name)\n",
    "        CB.tag.initializer = Constant(0.0)\n",
    "        output_tensor = theano.tensor.nnet.conv2d(input_tensor, CW) + CB.dimshuffle('x',0,'x','x')\n",
    "        return [CW,CB,output_tensor]\n",
    "\n",
    "class Layer_MaxPool():\n",
    "    def __init__(self,ds,st=None):\n",
    "        self.ds=ds\n",
    "        if st is not None:\n",
    "            self.st=st\n",
    "        else:\n",
    "            self.st=ds\n",
    "    \n",
    "    def apply(self,input_tensor,name):\n",
    "        return [None,None,theano.tensor.signal.downsample.max_pool_2d(input_tensor,ds=self.ds,st=self.st,ignore_border=True)]\n",
    "##TODO\n",
    "class Layer_LocalRespNorm():\n",
    "    def __init__(self, alpha = 1e-4, k=1, beta=0.75, n=5):\n",
    "        self.alpha=alpha\n",
    "        self.k=k;\n",
    "        self.beta=beta;\n",
    "        self.n=n;\n",
    "\n",
    "\n",
    "    def apply(self, input_tensor,name):\n",
    "        half = self.n // 2\n",
    "        input_squared=input_tensor**2\n",
    "        d0, d1, d2, d3 = input_tensor.shape\n",
    "        padded_input = theano.tensor.alloc(0., d0, d1+2*half, d2, d3)\n",
    "        input_squared = theano.tensor.set_subtensor(padded_input[:,half:half+d1,:,:], input_squared)\n",
    "        scale = theano.tensor.tensor4('S')\n",
    "        scale=input_tensor\n",
    "        #theano.tensor.mean(input_tensor, keepdims=True)\n",
    "        for i in xrange(self.n):\n",
    "            for j in xrange(self.n):\n",
    "                temporar=input_squared[:,i:i+d1,:,:]\n",
    "                scale += self.alpha * input_tensor[:,:,:]#input_squared[:,:,i:i+d2,j:j+d3]\n",
    "\n",
    "        \n",
    "        scale = scale ** self.beta\n",
    "\n",
    "        return [None,None,input_tensor / scale]\n",
    "\n",
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def apply(self,input_tensor):\n",
    "        return theano.tensor.flatten(input_tensor,2)\n",
    "\n",
    "##TODO\n",
    "class Layer_Dropout():\n",
    "    def __init__(self,drop_probab):\n",
    "        self.p=drop_probab\n",
    "        self.train_on=True\n",
    "    def apply(self,input_tensor,name,train_on):\n",
    "        inp_shape=input_tensor.tag.test_value.shape\n",
    "        print inp_shape\n",
    "        mask = srng.binomial(n=1, p=(1-self.p), size=(inp_shape[1],inp_shape[2]), dtype='float32')\n",
    "        mask=theano.shared(mask,name='Dmask'+name)\n",
    "        #print numpy.asarray(srng.uniform(0.0,1.0,(inp_shape[1],inp_shape[2]))>=self.p,dtype='float32')\n",
    "        \n",
    "        train_tensor=input_tensor*mask.dimshuffle('x',0)\n",
    "        output_tensor= theano.tensor.switch(theano.tensor.neq(train_on, 0), train_tensor, (1.-self.p)*input_tensor)\n",
    "        return[None,None,output_tensor]\n",
    "\n",
    "class Layer_BatchNorm():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def apply(self,input_tensor,name):\n",
    "        mean=theano.tensor.mean(input_tensor,axis=(0,2,3),keepdims=True)\n",
    "        std=theano.tensor.std(input_tensor,axis=(0,2,3),keepdims=True)\n",
    "        inp_shape=input_tensor.tag.test_value.shape\n",
    "        BNW = theano.shared(np.zeros(inp_shape[1],\n",
    "                                    dtype='float32'),name='BNW'+name)\n",
    "        BNW.tag.initializer= Constant(1)\n",
    "        BNB = theano.shared(np.zeros((inp_shape[1]), dtype='float32'),\n",
    "                    name='BNW'+name)\n",
    "        BNB.tag.initializer = IsotropicGaussian(0.01)\n",
    "        output_tensor=BNW.dimshuffle('x',0,'x','x')/(std+1e-3)*(input_tensor-mean)+BNB.dimshuffle('x',0,'x','x')\n",
    "        return [BNW,BNB,output_tensor]\n",
    "\n",
    "class Layer_RGBtoYUV():\n",
    "    def __init(self):\n",
    "        pass\n",
    "    \n",
    "    def apply(self,input_tensor,name):\n",
    "        r = input_tensor[:, 0, :, :]\n",
    "        g = input_tensor[:, 1, :, :]\n",
    "        b = input_tensor[:, 2, :, :]\n",
    "\n",
    "        y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "        u = -0.14713 * r - 0.28886 * g + 0.436 * b\n",
    "        v = 0.615 * r - 0.51499 * g - 0.10001 * b\n",
    "        \n",
    "        input_tensor = theano.tensor.set_subtensor(input_tensor[:, 0, :, :], y)\n",
    "        input_tensor = theano.tensor.set_subtensor(input_tensor[:, 1, :, :], u)\n",
    "        input_tensor = theano.tensor.set_subtensor(input_tensor[:, 2, :, :], v)\n",
    "        output_tensor=input_tensor\n",
    "        return [None,None,output_tensor]\n",
    "class Layer_Flip(): \n",
    "    def __init__(self,prob=0.2):\n",
    "        self.prob=prob\n",
    "        self.train_on=True\n",
    "    \n",
    "    def apply(self,input_tensor,name,train_on):\n",
    "        rv_u = srng.binomial(n=1,p=self.prob,size=(1,)).astype('float32')\n",
    "        f=theano.function([],rv_u)\n",
    "        coef=f()\n",
    "        train_tensor=input_tensor[:,:,:,::-1]*coef+(1-coef)*input_tensor\n",
    "        output_tensor= theano.tensor.switch(theano.tensor.neq(train_on, 0), train_tensor, input_tensor)\n",
    "        return [None,None,output_tensor]\n",
    "\n",
    "class Layer_GaussB(): \n",
    "    def __init__(self,filter_size,std=1.0):\n",
    "        self.std=std\n",
    "        self.filter_size=filter_size\n",
    "        self.train_on=True\n",
    "    \n",
    "    def apply(self,input_tensor,name):\n",
    "        half=self.filter_size//2\n",
    "        x = np.arange(-half,half+1)\n",
    "        y = np.arange(-half,half+1)\n",
    "        inp_shape=input_tensor.tag.test_value.shape\n",
    "        z = np.arange(0,inp_shape[1])\n",
    "        w = np.arange(0,inp_shape[1])\n",
    "        Z,W,X,Y=np.meshgrid(z,w,x,y)\n",
    "        gaussian=np.exp(-(X**2+Y**2)/(2*self.std**2))\n",
    "        gaussian=(inp_shape[1]**2*gaussian/np.sum(gaussian.reshape(-1))).astype(np.float32)\n",
    "        #print gaussian\n",
    "        G_kernel = theano.shared(gaussian)\n",
    "        train_tensor=theano.tensor.nnet.conv2d(input_tensor,G_kernel,border_mode='full')\n",
    "        output_tensor= theano.tensor.switch(theano.tensor.neq(train_on, 0), train_tensor, input_tensor)\n",
    "        return [None,None,output_tensor]\n",
    "\n",
    "class Layer_FullyCon():\n",
    "    def __init__(self,hidden_neurons,init_weight=None):\n",
    "        self.hidden_neurons=hidden_neurons\n",
    "        if init_weight is not None:\n",
    "            self.init_weight=init_weight\n",
    "        else:\n",
    "             self.init_weight=0.01\n",
    "        \n",
    "    def apply(self,input_tensor,name):\n",
    "        if input_tensor.tag.test_value.ndim!=2:\n",
    "            input_tensor=Flatten().apply(input_tensor)\n",
    "        \n",
    "        FW=theano.shared(np.zeros((input_tensor.tag.test_value.shape[1], self.hidden_neurons), dtype='float32'),\n",
    "                   name='FW'+name)\n",
    "        FW.tag.initializer = IsotropicGaussian(self.init_weight)\n",
    "        FB = theano.shared(np.zeros((self.hidden_neurons,), dtype='float32'),\n",
    "                    name='FB'+name)\n",
    "        FB.tag.initializer = Constant(0.0)\n",
    "        return [FW,FB,theano.tensor.dot(input_tensor,FW)+FB.dimshuffle('x',0)]\n",
    "    \n",
    "class Layer_SoftMax():\n",
    "    \"Creates Softmax Layer\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def apply(self,input_tensor,name):\n",
    "        return [None,None,theano.tensor.nnet.softmax(input_tensor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self,layers,X_test_value,train_on_value):\n",
    "        self.layers=layers\n",
    "        self.X_test_value=X_test_value\n",
    "        self.train_on_value=train_on_value\n",
    "    def apply(self,X,train_on):\n",
    "        model_parameters = []\n",
    "        X.tag.test_value=self.X_test_value\n",
    "        train_on.tag.test_value=self.train_on_value\n",
    "        for inx, layer in enumerate(self.layers):\n",
    "            name=str(inx)\n",
    "            if hasattr(layer,'train_on'):\n",
    "                FM,FB,newX=layer.apply(X,name,train_on)\n",
    "            else:\n",
    "                FM,FB,newX=layer.apply(X,name)\n",
    "            X=newX\n",
    "            print \"New X shape: %s\" % (X.tag.test_value.shape,)\n",
    "            if FM is not None:\n",
    "                model_parameters+= [FM,FB]\n",
    "        return X,model_parameters\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New X shape: (25, 3, 32, 32)\n",
      "New X shape: (25, 32, 28, 28)\n",
      "New X shape: (25, 32, 28, 28)\n",
      "New X shape: (25, 32, 14, 14)\n",
      "New X shape: (25, 32, 14, 14)\n",
      "New X shape: (25, 64, 10, 10)\n",
      "New X shape: (25, 64, 10, 10)\n",
      "New X shape: (25, 64, 5, 5)\n",
      "New X shape: (25, 64, 5, 5)\n",
      "New X shape: (25, 128, 1, 1)\n",
      "New X shape: (25, 128, 1, 1)\n",
      "New X shape: (25, 128, 1, 1)\n",
      "New X shape: (25, 512)\n",
      "New X shape: (25, 512)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4('X')\n",
    "train_on =theano.tensor.iscalar('train_on') \n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "\n",
    "#The tag values are useful during debugging the creation of Theano graphs\n",
    "\n",
    "X_test_value, Y_test_value = next(cifar_train_stream.get_epoch_iterator())\n",
    "#\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "#\n",
    "theano.config.compute_test_value = 'raise' # Enable the computation of test values\n",
    "Y.tag.test_value = Y_test_value\n",
    "train_on.tag.test_value=1\n",
    "# this list will hold all parameters of the network\n",
    "\n",
    "model_parameters = []\n",
    "theano.config.on_unused_input='ignore'\n",
    "CNN=Network(\n",
    "    [\n",
    "     Layer_RGBtoYUV(),\n",
    "     Layer_Conv(32,5,0.001),\n",
    "     Layer_ReLU(),\n",
    "     Layer_MaxPool((2,2)),\n",
    "     Layer_BatchNorm(),\n",
    "     Layer_Conv(64,5,0.001),\n",
    "     Layer_ReLU(),\n",
    "     Layer_MaxPool((2,2)),\n",
    "     Layer_BatchNorm(),\n",
    "     Layer_Conv(128,5,0.01),\n",
    "     Layer_ReLU(),\n",
    "     Layer_MaxPool((1,1)),\n",
    "     Layer_FullyCon(512,0.05),\n",
    "     Layer_SoftMax()\n",
    "        \n",
    "    ],X_test_value,1)\n",
    "log_probs,model_parameters = CNN.apply(X,train_on)\n",
    "predictions =theano.tensor.argmax(log_probs, axis=1)\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p is not None  and p.name[1]=='W':\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "#At this point stop computing test values\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = []\n",
    "lrate = theano.tensor.scalar('lrate',dtype='float32')\n",
    "momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in model_parameters]\n",
    "\n",
    "for p,g,v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "train_step = theano.function([X,Y,train_on,lrate,momentum],[cost, error_rate, nll, weight_decay],updates=updates)\n",
    "#each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X,train_on], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X,0)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "\n",
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 32100, batch loss 0.521083, batch nll 0.205898, batch error rate 12.000000%\n",
      "At minibatch 32200, batch loss 0.496519, batch nll 0.181341, batch error rate 12.000000%\n",
      "At minibatch 32300, batch loss 0.457640, batch nll 0.142469, batch error rate 4.000000%\n",
      "At minibatch 32400, batch loss 0.429968, batch nll 0.114804, batch error rate 0.000000%\n",
      "At minibatch 32500, batch loss 0.393599, batch nll 0.078442, batch error rate 0.000000%\n",
      "At minibatch 32600, batch loss 0.564631, batch nll 0.249481, batch error rate 8.000000%\n",
      "At minibatch 32700, batch loss 0.509124, batch nll 0.193981, batch error rate 8.000000%\n",
      "At minibatch 32800, batch loss 0.475935, batch nll 0.160799, batch error rate 4.000000%\n",
      "At minibatch 32900, batch loss 0.452983, batch nll 0.137854, batch error rate 4.000000%\n",
      "At minibatch 33000, batch loss 0.353976, batch nll 0.038854, batch error rate 0.000000%\n",
      "At minibatch 33100, batch loss 0.448796, batch nll 0.133681, batch error rate 4.000000%\n",
      "At minibatch 33200, batch loss 0.405584, batch nll 0.090476, batch error rate 4.000000%\n",
      "At minibatch 33300, batch loss 0.421259, batch nll 0.106158, batch error rate 0.000000%\n",
      "At minibatch 33400, batch loss 0.456454, batch nll 0.141360, batch error rate 4.000000%\n",
      "At minibatch 33500, batch loss 0.345913, batch nll 0.030827, batch error rate 0.000000%\n",
      "At minibatch 33600, batch loss 0.424156, batch nll 0.109077, batch error rate 0.000000%\n",
      "After epoch 22: valid_err_rate: 24.580000% currently going to do 55 epochs\n",
      "After epoch 22: averaged train_err_rate: 2.476548% averaged train nll: 0.115152 averaged train loss: 0.430284\n",
      "At minibatch 33700, batch loss 0.382948, batch nll 0.067875, batch error rate 0.000000%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-3c44e1f8651d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwdec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#print [p.get_value().ravel()[:10] for p in model_parameters]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/pio/os/anaconda/lib/python2.7/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "while e<number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in cifar_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        #print i\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 #* K / np.maximum(K, i)\n",
    "        #try 1e-3, 1e-4 after e\n",
    "        if e>10:\n",
    "            lrate=1e-4\n",
    "        if e>20:\n",
    "            lrate=1e-5\n",
    "        if e>40:\n",
    "            lrate=1e-6\n",
    "        momentum=0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch,1, lrate, momentum)\n",
    "        \n",
    "        #print [p.get_value().ravel()[:10] for p in model_parameters]\n",
    "        #print [p.get_value().ravel()[:10] for p in velocities]\n",
    "        \n",
    "        \n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs)\n",
    "\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 18\n",
      "Test error rate is 24.250000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9ad36f2410>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxGvUKhkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b0a40bf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" %(compute_error_rate(cifar_test_stream)*100.0,)\n",
    "\n",
    "subplot(2,1,1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:,0], train_nll_a[:,1], label='batch train nll')\n",
    "legend()\n",
    "\n",
    "subplot(2,1,2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:,0], train_erros_a[:,1], label='batch train error rate')\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:,0], validation_errors_a[:,1], label='validation error rate', color='r')\n",
    "ylim(0,0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How do the filters in the first layer look like?\n",
    "\n",
    "plot_mat(CW1.get_value(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iii = predict.maker.inputs[0]\n",
    "X = iii.variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a function that shows how the network processes an image\n",
    "\n",
    "middle_layers_computer = theano.function([X], [\n",
    "        X,\n",
    "        after_C1,\n",
    "        after_P1,\n",
    "        after_C2,\n",
    "        after_P2\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_num=4\n",
    "\n",
    "middle_layers = middle_layers_computer(X_test_value[img_num:img_num+1])\n",
    "\n",
    "for ml, name in zip(middle_layers, ['X', 'C1', 'P1', 'C2', 'P2']):\n",
    "    plot_mat(ml.transpose(1,0,2,3), cmap='gray')\n",
    "    title(name)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
