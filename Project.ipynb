{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 24 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 24 days\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "#note: this requires the starter code for the assignments!\n",
    "from common.plotting import plot_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building dataset:\n",
    "\n",
    "[Customizable: trainset size]\n",
    "\n",
    "Initializing all data picking train/validation sets ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "#trainset [0,50k]\n",
    "trainset=40000\n",
    "cifar_train = CIFAR10((\"train\",), subset=slice(None,trainset))\n",
    "\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "cifar_train_stream = DataStream.default_stream(\n",
    "    cifar_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar_train.num_examples, 25))\n",
    "cifar_validation = CIFAR10((\"train\",), subset=slice(trainset, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "cifar_validation_stream = DataStream.default_stream(\n",
    "    cifar_validation, iteration_scheme=SequentialScheme(cifar_validation.num_examples, 25))\n",
    "cifar_test = CIFAR10((\"test\",))\n",
    "cifar_test_stream = DataStream.default_stream(\n",
    "    cifar_test, iteration_scheme=SequentialScheme(cifar_test.num_examples, 25))\n",
    "\n",
    "print \"The streams return batches containing %s\" % (cifar_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "srng = theano.tensor.shared_randomstreams.RandomStreams(rng.randint(999999))\n",
    "class Layer_ReLU():\n",
    "    \"Creates ReLU layer\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def apply(self,input_tensor,name):\n",
    "        return [None,None,theano.tensor.maximum(0.0,input_tensor)]\n",
    "\n",
    "class Layer_Conv():\n",
    "    def __init__(self,num_filters,filter_size,init_weight=None,padding=0):\n",
    "            self.num_filters=num_filters\n",
    "            self.filter_size=filter_size\n",
    "            self.padding=padding\n",
    "            if init_weight is not None:\n",
    "                self.init_weight=init_weight\n",
    "            else:\n",
    "                 self.init_weight=0.01\n",
    "    \n",
    "    def apply(self,input_tensor,name):\n",
    "        \n",
    "        CW = theano.shared(np.zeros((self.num_filters,input_tensor.tag.test_value.shape[1],self.filter_size,self.filter_size),\n",
    "                                    dtype='float32'),name='CW'+name)\n",
    "        CW.tag.initializer= IsotropicGaussian(self.init_weight)\n",
    "        CB = theano.shared(np.zeros((self.num_filters), dtype='float32'),\n",
    "                    name='CB'+name)\n",
    "        CB.tag.initializer = Constant(0.0)\n",
    "        output_tensor = theano.tensor.nnet.conv2d(input_tensor, CW) + CB.dimshuffle('x',0,'x','x')\n",
    "        return [CW,CB,output_tensor]\n",
    "\n",
    "class Layer_MaxPool():\n",
    "    def __init__(self,ds,st=None):\n",
    "        self.ds=ds\n",
    "        if st is not None:\n",
    "            self.st=st\n",
    "        else:\n",
    "            self.st=ds\n",
    "    \n",
    "    def apply(self,input_tensor,name):\n",
    "        return [None,None,theano.tensor.signal.downsample.max_pool_2d(input_tensor,ds=self.ds,st=self.st,ignore_border=True)]\n",
    "##Doesn't improve anything\n",
    "class Layer_LocalRespNorm():\n",
    "    def __init__(self, alpha = 1e-4, k=1, beta=0.75, n=5):\n",
    "        self.alpha=alpha\n",
    "        self.k=k;\n",
    "        self.beta=beta;\n",
    "        self.n=n;\n",
    "\n",
    "\n",
    "    def apply(self, input_tensor,name):\n",
    "        half = self.n // 2\n",
    "        input_squared=input_tensor**2\n",
    "        d0, d1, d2, d3 = input_tensor.shape\n",
    "        padded_input = theano.tensor.alloc(0., d0, d1+2*half, d2, d3)\n",
    "        input_squared = theano.tensor.set_subtensor(padded_input[:,half:half+d1,:,:], input_squared)\n",
    "        scale = theano.tensor.tensor4('S')\n",
    "        scale=input_tensor\n",
    "        #theano.tensor.mean(input_tensor, keepdims=True)\n",
    "        for i in xrange(self.n):\n",
    "            for j in xrange(self.n):\n",
    "                temporar=input_squared[:,i:i+d1,:,:]\n",
    "                scale += self.alpha * input_tensor[:,:,:]#input_squared[:,:,i:i+d2,j:j+d3]\n",
    "\n",
    "        \n",
    "        scale = scale ** self.beta\n",
    "\n",
    "        return [None,None,input_tensor / scale]\n",
    "\n",
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def apply(self,input_tensor):\n",
    "        return theano.tensor.flatten(input_tensor,2)\n",
    "\n",
    "##Doesn't improve anything\n",
    "class Layer_Dropout():\n",
    "    def __init__(self,drop_probab):\n",
    "        self.p=drop_probab\n",
    "        self.train_on=True\n",
    "    def apply(self,input_tensor,name,train_on):\n",
    "        inp_shape=input_tensor.tag.test_value.shape\n",
    "        print inp_shape\n",
    "        mask = srng.binomial(n=1, p=(1-self.p), size=(inp_shape[1],inp_shape[2]), dtype='float32')\n",
    "        mask=theano.shared(mask,name='Dmask'+name)\n",
    "        #print numpy.asarray(srng.uniform(0.0,1.0,(inp_shape[1],inp_shape[2]))>=self.p,dtype='float32')\n",
    "        \n",
    "        train_tensor=input_tensor*mask.dimshuffle('x',0)\n",
    "        output_tensor= theano.tensor.switch(theano.tensor.neq(train_on, 0), train_tensor, (1.-self.p)*input_tensor)\n",
    "        return[None,None,output_tensor]\n",
    "\n",
    "class Layer_BatchNorm():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def apply(self,input_tensor,name):\n",
    "        mean=theano.tensor.mean(input_tensor,axis=(0,2,3),keepdims=True)\n",
    "        std=theano.tensor.std(input_tensor,axis=(0,2,3),keepdims=True)\n",
    "        inp_shape=input_tensor.tag.test_value.shape\n",
    "        BNW = theano.shared(np.zeros(inp_shape[1],\n",
    "                                    dtype='float32'),name='BNW'+name)\n",
    "        BNW.tag.initializer= Constant(1)\n",
    "        BNB = theano.shared(np.zeros((inp_shape[1]), dtype='float32'),\n",
    "                    name='BNW'+name)\n",
    "        BNB.tag.initializer = IsotropicGaussian(0.01)\n",
    "        output_tensor=BNW.dimshuffle('x',0,'x','x')/(std+1e-3)*(input_tensor-mean)+BNB.dimshuffle('x',0,'x','x')\n",
    "        return [BNW,BNB,output_tensor]\n",
    "\n",
    "class Layer_RGBtoYUV():\n",
    "    def __init(self):\n",
    "        pass\n",
    "    \n",
    "    def apply(self,input_tensor,name):\n",
    "        r = input_tensor[:, 0, :, :]\n",
    "        g = input_tensor[:, 1, :, :]\n",
    "        b = input_tensor[:, 2, :, :]\n",
    "\n",
    "        y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "        u = -0.14713 * r - 0.28886 * g + 0.436 * b\n",
    "        v = 0.615 * r - 0.51499 * g - 0.10001 * b\n",
    "        \n",
    "        input_tensor = theano.tensor.set_subtensor(input_tensor[:, 0, :, :], y)\n",
    "        input_tensor = theano.tensor.set_subtensor(input_tensor[:, 1, :, :], u)\n",
    "        input_tensor = theano.tensor.set_subtensor(input_tensor[:, 2, :, :], v)\n",
    "        output_tensor=input_tensor\n",
    "        return [None,None,output_tensor]\n",
    "class Layer_Flip(): \n",
    "    def __init__(self,prob=0.2):\n",
    "        self.prob=prob\n",
    "        self.train_on=True\n",
    "    \n",
    "    def apply(self,input_tensor,name,train_on):\n",
    "        rv_u = srng.binomial(n=1,p=self.prob,size=(1,)).astype('float32')\n",
    "        f=theano.function([],rv_u)\n",
    "        coef=f()\n",
    "        train_tensor=input_tensor[:,:,:,::-1]*coef+(1-coef)*input_tensor\n",
    "        output_tensor= theano.tensor.switch(theano.tensor.neq(train_on, 0), train_tensor, input_tensor)\n",
    "        return [None,None,output_tensor]\n",
    "\n",
    "class Layer_GaussB(): \n",
    "    def __init__(self,filter_size,std=1.0):\n",
    "        self.std=std\n",
    "        self.filter_size=filter_size\n",
    "        self.train_on=True\n",
    "    \n",
    "    def apply(self,input_tensor,name):\n",
    "        half=self.filter_size//2\n",
    "        x = np.arange(-half,half+1)\n",
    "        y = np.arange(-half,half+1)\n",
    "        inp_shape=input_tensor.tag.test_value.shape\n",
    "        z = np.arange(0,inp_shape[1])\n",
    "        w = np.arange(0,inp_shape[1])\n",
    "        Z,W,X,Y=np.meshgrid(z,w,x,y)\n",
    "        gaussian=np.exp(-(X**2+Y**2)/(2*self.std**2))\n",
    "        gaussian=(inp_shape[1]**2*gaussian/np.sum(gaussian.reshape(-1))).astype(np.float32)\n",
    "        #print gaussian\n",
    "        G_kernel = theano.shared(gaussian)\n",
    "        train_tensor=theano.tensor.nnet.conv2d(input_tensor,G_kernel,border_mode='full')\n",
    "        output_tensor= theano.tensor.switch(theano.tensor.neq(train_on, 0), train_tensor, input_tensor)\n",
    "        return [None,None,output_tensor]\n",
    "\n",
    "class Layer_FullyCon():\n",
    "    def __init__(self,hidden_neurons,init_weight=None):\n",
    "        self.hidden_neurons=hidden_neurons\n",
    "        if init_weight is not None:\n",
    "            self.init_weight=init_weight\n",
    "        else:\n",
    "             self.init_weight=0.01\n",
    "        \n",
    "    def apply(self,input_tensor,name):\n",
    "        if input_tensor.tag.test_value.ndim!=2:\n",
    "            input_tensor=Flatten().apply(input_tensor)\n",
    "        \n",
    "        FW=theano.shared(np.zeros((input_tensor.tag.test_value.shape[1], self.hidden_neurons), dtype='float32'),\n",
    "                   name='FW'+name)\n",
    "        FW.tag.initializer = IsotropicGaussian(self.init_weight)\n",
    "        FB = theano.shared(np.zeros((self.hidden_neurons,), dtype='float32'),\n",
    "                    name='FB'+name)\n",
    "        FB.tag.initializer = Constant(0.0)\n",
    "        return [FW,FB,theano.tensor.dot(input_tensor,FW)+FB.dimshuffle('x',0)]\n",
    "    \n",
    "class Layer_SoftMax():\n",
    "    \"Creates Softmax Layer\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def apply(self,input_tensor,name):\n",
    "        return [None,None,theano.tensor.nnet.softmax(input_tensor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self,layers,X_test_value,train_on_value):\n",
    "        self.layers=layers\n",
    "        self.X_test_value=X_test_value\n",
    "        self.train_on_value=train_on_value\n",
    "    def apply(self,X,train_on):\n",
    "        model_parameters = []\n",
    "        X.tag.test_value=self.X_test_value\n",
    "        train_on.tag.test_value=self.train_on_value\n",
    "        for inx, layer in enumerate(self.layers):\n",
    "            name=str(inx)\n",
    "            if hasattr(layer,'train_on'):\n",
    "                FM,FB,newX=layer.apply(X,name,train_on)\n",
    "            else:\n",
    "                FM,FB,newX=layer.apply(X,name)\n",
    "            X=newX\n",
    "            print \"New X shape: %s\" % (X.tag.test_value.shape,)\n",
    "            if FM is not None:\n",
    "                model_parameters+= [FM,FB]\n",
    "        return X,model_parameters\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New X shape: (25, 3, 32, 32)\n",
      "New X shape: (25, 32, 28, 28)\n",
      "New X shape: (25, 32, 28, 28)\n",
      "New X shape: (25, 32, 14, 14)\n",
      "New X shape: (25, 32, 14, 14)\n",
      "New X shape: (25, 64, 10, 10)\n",
      "New X shape: (25, 64, 10, 10)\n",
      "New X shape: (25, 64, 5, 5)\n",
      "New X shape: (25, 64, 5, 5)\n",
      "New X shape: (25, 128, 1, 1)\n",
      "New X shape: (25, 128, 1, 1)\n",
      "New X shape: (25, 512)\n",
      "New X shape: (25, 512)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4('X')\n",
    "train_on =theano.tensor.iscalar('train_on') \n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "\n",
    "#The tag values are useful during debugging the creation of Theano graphs\n",
    "\n",
    "X_test_value, Y_test_value = next(cifar_train_stream.get_epoch_iterator())\n",
    "#\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "#\n",
    "theano.config.compute_test_value = 'raise' # Enable the computation of test values\n",
    "Y.tag.test_value = Y_test_value\n",
    "train_on.tag.test_value=1\n",
    "# this list will hold all parameters of the network\n",
    "\n",
    "model_parameters = []\n",
    "theano.config.on_unused_input='ignore'\n",
    "CNN=Network(\n",
    "    [\n",
    "     Layer_RGBtoYUV(),\n",
    "     Layer_Conv(32,5,0.001),\n",
    "     Layer_ReLU(),\n",
    "     Layer_MaxPool((2,2)),\n",
    "     Layer_BatchNorm(),\n",
    "     Layer_Conv(64,5,0.001),\n",
    "     Layer_ReLU(),\n",
    "     Layer_MaxPool((2,2)),\n",
    "     Layer_BatchNorm(),\n",
    "     Layer_Conv(128,5,0.01),\n",
    "     Layer_ReLU(),\n",
    "     Layer_FullyCon(512,0.05),\n",
    "     Layer_SoftMax()\n",
    "        \n",
    "    ],X_test_value,1)\n",
    "log_probs,model_parameters = CNN.apply(X,train_on)\n",
    "predictions =theano.tensor.argmax(log_probs, axis=1)\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p is not None  and p.name[1]=='W':\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "#At this point stop computing test values\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = []\n",
    "lrate = theano.tensor.scalar('lrate',dtype='float32')\n",
    "momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in model_parameters]\n",
    "\n",
    "for p,g,v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "train_step = theano.function([X,Y,train_on,lrate,momentum],[cost, error_rate, nll, weight_decay],updates=updates)\n",
    "#each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X,train_on], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X,0)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "\n",
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 1.680315, batch nll 1.482210, batch error rate 52.000000%\n",
      "At minibatch 200, batch loss 1.842325, batch nll 1.642967, batch error rate 56.000000%\n",
      "At minibatch 300, batch loss 1.841623, batch nll 1.642163, batch error rate 56.000000%\n",
      "At minibatch 400, batch loss 1.565097, batch nll 1.365087, batch error rate 52.000000%\n",
      "At minibatch 500, batch loss 1.573486, batch nll 1.373355, batch error rate 52.000000%\n",
      "At minibatch 600, batch loss 1.610164, batch nll 1.410179, batch error rate 44.000000%\n",
      "At minibatch 700, batch loss 1.408688, batch nll 1.208551, batch error rate 40.000000%\n",
      "At minibatch 800, batch loss 1.448850, batch nll 1.248818, batch error rate 36.000000%\n",
      "At minibatch 900, batch loss 1.628445, batch nll 1.428456, batch error rate 60.000000%\n",
      "At minibatch 1000, batch loss 1.436950, batch nll 1.236899, batch error rate 48.000000%\n",
      "At minibatch 1100, batch loss 1.720807, batch nll 1.520697, batch error rate 52.000000%\n",
      "At minibatch 1200, batch loss 1.282075, batch nll 1.081511, batch error rate 48.000000%\n",
      "At minibatch 1300, batch loss 1.748674, batch nll 1.548257, batch error rate 60.000000%\n",
      "At minibatch 1400, batch loss 1.141579, batch nll 0.941201, batch error rate 40.000000%\n",
      "At minibatch 1500, batch loss 1.357809, batch nll 1.157616, batch error rate 52.000000%\n",
      "At minibatch 1600, batch loss 0.918597, batch nll 0.718146, batch error rate 24.000000%\n",
      "After epoch 1: valid_err_rate: 40.340000% currently going to do 4 epochs\n",
      "After epoch 1: averaged train_err_rate: 51.075000% averaged train nll: 1.483599 averaged train loss: 1.683251\n",
      "At minibatch 1700, batch loss 1.424921, batch nll 1.223964, batch error rate 40.000000%\n",
      "At minibatch 1800, batch loss 1.141081, batch nll 0.939769, batch error rate 36.000000%\n",
      "At minibatch 1900, batch loss 1.227016, batch nll 1.025444, batch error rate 40.000000%\n",
      "At minibatch 2000, batch loss 1.574131, batch nll 1.372060, batch error rate 48.000000%\n",
      "At minibatch 2100, batch loss 1.143234, batch nll 0.940875, batch error rate 36.000000%\n",
      "At minibatch 2200, batch loss 1.517635, batch nll 1.314850, batch error rate 52.000000%\n",
      "At minibatch 2300, batch loss 1.671756, batch nll 1.468757, batch error rate 40.000000%\n",
      "At minibatch 2400, batch loss 1.007017, batch nll 0.804066, batch error rate 24.000000%\n",
      "At minibatch 2500, batch loss 1.585502, batch nll 1.382281, batch error rate 52.000000%\n",
      "At minibatch 2600, batch loss 1.390569, batch nll 1.186802, batch error rate 32.000000%\n",
      "At minibatch 2700, batch loss 1.112619, batch nll 0.908580, batch error rate 24.000000%\n",
      "At minibatch 2800, batch loss 1.159944, batch nll 0.955651, batch error rate 24.000000%\n",
      "At minibatch 2900, batch loss 1.574922, batch nll 1.370548, batch error rate 48.000000%\n",
      "At minibatch 3000, batch loss 1.134572, batch nll 0.930082, batch error rate 32.000000%\n",
      "At minibatch 3100, batch loss 1.597901, batch nll 1.393044, batch error rate 36.000000%\n",
      "At minibatch 3200, batch loss 0.850857, batch nll 0.645717, batch error rate 16.000000%\n",
      "After epoch 2: valid_err_rate: 32.500000% currently going to do 7 epochs\n",
      "After epoch 2: averaged train_err_rate: 36.660000% averaged train nll: 1.044756 averaged train loss: 1.247822\n",
      "At minibatch 3300, batch loss 1.066332, batch nll 0.860136, batch error rate 36.000000%\n",
      "At minibatch 3400, batch loss 0.689133, batch nll 0.481802, batch error rate 8.000000%\n",
      "At minibatch 3500, batch loss 1.341375, batch nll 1.132926, batch error rate 40.000000%\n",
      "At minibatch 3600, batch loss 1.390884, batch nll 1.181628, batch error rate 40.000000%\n",
      "At minibatch 3700, batch loss 1.147089, batch nll 0.937035, batch error rate 36.000000%\n",
      "At minibatch 3800, batch loss 1.481867, batch nll 1.271231, batch error rate 48.000000%\n",
      "At minibatch 3900, batch loss 1.204417, batch nll 0.993252, batch error rate 36.000000%\n",
      "At minibatch 4000, batch loss 1.208202, batch nll 0.996449, batch error rate 32.000000%\n",
      "At minibatch 4100, batch loss 0.869042, batch nll 0.657118, batch error rate 28.000000%\n",
      "At minibatch 4200, batch loss 0.906302, batch nll 0.693762, batch error rate 20.000000%\n",
      "At minibatch 4300, batch loss 0.922101, batch nll 0.708933, batch error rate 28.000000%\n",
      "At minibatch 4400, batch loss 0.918879, batch nll 0.705398, batch error rate 24.000000%\n",
      "At minibatch 4500, batch loss 1.054961, batch nll 0.841060, batch error rate 24.000000%\n",
      "At minibatch 4600, batch loss 0.991460, batch nll 0.777562, batch error rate 28.000000%\n",
      "At minibatch 4700, batch loss 1.010574, batch nll 0.796176, batch error rate 28.000000%\n",
      "At minibatch 4800, batch loss 1.349900, batch nll 1.135380, batch error rate 44.000000%\n",
      "After epoch 3: valid_err_rate: 33.200000% currently going to do 7 epochs\n",
      "After epoch 3: averaged train_err_rate: 30.325000% averaged train nll: 0.869628 averaged train loss: 1.080767\n",
      "At minibatch 4900, batch loss 0.703310, batch nll 0.487377, batch error rate 16.000000%\n",
      "At minibatch 5000, batch loss 1.188311, batch nll 0.971275, batch error rate 40.000000%\n",
      "At minibatch 5100, batch loss 0.663505, batch nll 0.445231, batch error rate 12.000000%\n",
      "At minibatch 5200, batch loss 1.295544, batch nll 1.076052, batch error rate 40.000000%\n",
      "At minibatch 5300, batch loss 1.113809, batch nll 0.893356, batch error rate 32.000000%\n",
      "At minibatch 5400, batch loss 0.903608, batch nll 0.682399, batch error rate 24.000000%\n",
      "At minibatch 5500, batch loss 0.914614, batch nll 0.692792, batch error rate 32.000000%\n",
      "At minibatch 5600, batch loss 0.778556, batch nll 0.555751, batch error rate 20.000000%\n",
      "At minibatch 5700, batch loss 0.928118, batch nll 0.704467, batch error rate 24.000000%\n",
      "At minibatch 5800, batch loss 0.572661, batch nll 0.348406, batch error rate 12.000000%\n",
      "At minibatch 5900, batch loss 0.844024, batch nll 0.619275, batch error rate 16.000000%\n",
      "At minibatch 6000, batch loss 0.835542, batch nll 0.610625, batch error rate 16.000000%\n",
      "At minibatch 6100, batch loss 0.584792, batch nll 0.359188, batch error rate 12.000000%\n",
      "At minibatch 6200, batch loss 0.766832, batch nll 0.540506, batch error rate 24.000000%\n",
      "At minibatch 6300, batch loss 0.870458, batch nll 0.644175, batch error rate 28.000000%\n",
      "At minibatch 6400, batch loss 1.285372, batch nll 1.058348, batch error rate 36.000000%\n",
      "After epoch 4: valid_err_rate: 30.100000% currently going to do 13 epochs\n",
      "After epoch 4: averaged train_err_rate: 26.205000% averaged train nll: 0.746722 averaged train loss: 0.968846\n",
      "At minibatch 6500, batch loss 1.216998, batch nll 0.988848, batch error rate 32.000000%\n",
      "At minibatch 6600, batch loss 1.114352, batch nll 0.884924, batch error rate 32.000000%\n",
      "At minibatch 6700, batch loss 0.601941, batch nll 0.371440, batch error rate 4.000000%\n",
      "At minibatch 6800, batch loss 0.812863, batch nll 0.580948, batch error rate 20.000000%\n",
      "At minibatch 6900, batch loss 1.148478, batch nll 0.915592, batch error rate 32.000000%\n",
      "At minibatch 7000, batch loss 1.242969, batch nll 1.009082, batch error rate 20.000000%\n",
      "At minibatch 7100, batch loss 0.976546, batch nll 0.741605, batch error rate 20.000000%\n",
      "At minibatch 7200, batch loss 1.115025, batch nll 0.878925, batch error rate 20.000000%\n",
      "At minibatch 7300, batch loss 0.639530, batch nll 0.402629, batch error rate 16.000000%\n",
      "At minibatch 7400, batch loss 0.863090, batch nll 0.625341, batch error rate 24.000000%\n",
      "At minibatch 7500, batch loss 0.866956, batch nll 0.628309, batch error rate 24.000000%\n",
      "At minibatch 7600, batch loss 0.704590, batch nll 0.465156, batch error rate 12.000000%\n",
      "At minibatch 7700, batch loss 0.693877, batch nll 0.453780, batch error rate 8.000000%\n",
      "At minibatch 7800, batch loss 0.887117, batch nll 0.646041, batch error rate 24.000000%\n",
      "At minibatch 7900, batch loss 1.179670, batch nll 0.937477, batch error rate 36.000000%\n",
      "At minibatch 8000, batch loss 1.012225, batch nll 0.769571, batch error rate 32.000000%\n",
      "After epoch 5: valid_err_rate: 29.740000% currently going to do 16 epochs\n",
      "After epoch 5: averaged train_err_rate: 23.095000% averaged train nll: 0.661119 averaged train loss: 0.896667\n",
      "At minibatch 8100, batch loss 1.091752, batch nll 0.847492, batch error rate 36.000000%\n",
      "At minibatch 8200, batch loss 0.478504, batch nll 0.232805, batch error rate 8.000000%\n",
      "At minibatch 8300, batch loss 0.539039, batch nll 0.292491, batch error rate 16.000000%\n",
      "At minibatch 8400, batch loss 0.875331, batch nll 0.627762, batch error rate 24.000000%\n",
      "At minibatch 8500, batch loss 0.808324, batch nll 0.559499, batch error rate 20.000000%\n",
      "At minibatch 8600, batch loss 0.613172, batch nll 0.363190, batch error rate 12.000000%\n",
      "At minibatch 8700, batch loss 0.701396, batch nll 0.450153, batch error rate 8.000000%\n",
      "At minibatch 8800, batch loss 0.762495, batch nll 0.510230, batch error rate 16.000000%\n",
      "At minibatch 8900, batch loss 1.121162, batch nll 0.867784, batch error rate 28.000000%\n",
      "At minibatch 9000, batch loss 1.037375, batch nll 0.782912, batch error rate 32.000000%\n",
      "At minibatch 9100, batch loss 1.097176, batch nll 0.841897, batch error rate 28.000000%\n",
      "At minibatch 9200, batch loss 0.663882, batch nll 0.407852, batch error rate 8.000000%\n",
      "At minibatch 9300, batch loss 1.012117, batch nll 0.755041, batch error rate 24.000000%\n",
      "At minibatch 9400, batch loss 1.155224, batch nll 0.897181, batch error rate 20.000000%\n",
      "At minibatch 9500, batch loss 1.359334, batch nll 1.100413, batch error rate 32.000000%\n",
      "At minibatch 9600, batch loss 1.208981, batch nll 0.949202, batch error rate 40.000000%\n",
      "After epoch 6: valid_err_rate: 28.480000% currently going to do 19 epochs\n",
      "After epoch 6: averaged train_err_rate: 20.287500% averaged train nll: 0.586454 averaged train loss: 0.838389\n",
      "At minibatch 9700, batch loss 0.923811, batch nll 0.662513, batch error rate 28.000000%\n",
      "At minibatch 9800, batch loss 0.603604, batch nll 0.341168, batch error rate 16.000000%\n",
      "At minibatch 9900, batch loss 0.804598, batch nll 0.540841, batch error rate 20.000000%\n",
      "At minibatch 10000, batch loss 0.902295, batch nll 0.637244, batch error rate 28.000000%\n",
      "At minibatch 10100, batch loss 1.008535, batch nll 0.742371, batch error rate 32.000000%\n",
      "At minibatch 10200, batch loss 1.003676, batch nll 0.736443, batch error rate 20.000000%\n",
      "At minibatch 10300, batch loss 0.818242, batch nll 0.549721, batch error rate 16.000000%\n",
      "At minibatch 10400, batch loss 0.648364, batch nll 0.378718, batch error rate 12.000000%\n",
      "At minibatch 10500, batch loss 1.093209, batch nll 0.822243, batch error rate 32.000000%\n",
      "At minibatch 10600, batch loss 0.864132, batch nll 0.592133, batch error rate 20.000000%\n",
      "At minibatch 10700, batch loss 0.631423, batch nll 0.358442, batch error rate 12.000000%\n",
      "At minibatch 10800, batch loss 0.640136, batch nll 0.365935, batch error rate 12.000000%\n",
      "At minibatch 10900, batch loss 1.115223, batch nll 0.840053, batch error rate 36.000000%\n",
      "At minibatch 11000, batch loss 0.874498, batch nll 0.598273, batch error rate 16.000000%\n",
      "At minibatch 11100, batch loss 0.847448, batch nll 0.570527, batch error rate 16.000000%\n",
      "At minibatch 11200, batch loss 0.691338, batch nll 0.413538, batch error rate 16.000000%\n",
      "After epoch 7: valid_err_rate: 29.470000% currently going to do 19 epochs\n",
      "After epoch 7: averaged train_err_rate: 18.755000% averaged train nll: 0.528659 averaged train loss: 0.798119\n",
      "At minibatch 11300, batch loss 0.650859, batch nll 0.372028, batch error rate 16.000000%\n",
      "At minibatch 11400, batch loss 0.494450, batch nll 0.214707, batch error rate 4.000000%\n",
      "At minibatch 11500, batch loss 0.542068, batch nll 0.261581, batch error rate 8.000000%\n",
      "At minibatch 11600, batch loss 0.743435, batch nll 0.461714, batch error rate 20.000000%\n",
      "At minibatch 11700, batch loss 0.756316, batch nll 0.473257, batch error rate 20.000000%\n",
      "At minibatch 11800, batch loss 0.876252, batch nll 0.592189, batch error rate 28.000000%\n",
      "At minibatch 11900, batch loss 0.663438, batch nll 0.378686, batch error rate 12.000000%\n",
      "At minibatch 12000, batch loss 1.140253, batch nll 0.854281, batch error rate 36.000000%\n",
      "At minibatch 12100, batch loss 0.726948, batch nll 0.439749, batch error rate 12.000000%\n",
      "At minibatch 12200, batch loss 0.945505, batch nll 0.656998, batch error rate 20.000000%\n",
      "At minibatch 12300, batch loss 0.914624, batch nll 0.624991, batch error rate 20.000000%\n",
      "At minibatch 12400, batch loss 1.275015, batch nll 0.984278, batch error rate 40.000000%\n",
      "At minibatch 12500, batch loss 0.656819, batch nll 0.364823, batch error rate 16.000000%\n",
      "At minibatch 12600, batch loss 0.755836, batch nll 0.462968, batch error rate 20.000000%\n",
      "At minibatch 12700, batch loss 0.683132, batch nll 0.389156, batch error rate 12.000000%\n",
      "At minibatch 12800, batch loss 0.832190, batch nll 0.537283, batch error rate 16.000000%\n",
      "After epoch 8: valid_err_rate: 29.830000% currently going to do 19 epochs\n",
      "After epoch 8: averaged train_err_rate: 16.370000% averaged train nll: 0.470004 averaged train loss: 0.756236\n",
      "At minibatch 12900, batch loss 0.676987, batch nll 0.381222, batch error rate 12.000000%\n",
      "At minibatch 13000, batch loss 0.610501, batch nll 0.313921, batch error rate 16.000000%\n",
      "At minibatch 13100, batch loss 0.555487, batch nll 0.258414, batch error rate 16.000000%\n",
      "At minibatch 13200, batch loss 0.666769, batch nll 0.368822, batch error rate 12.000000%\n",
      "At minibatch 13300, batch loss 0.614369, batch nll 0.315509, batch error rate 16.000000%\n",
      "At minibatch 13400, batch loss 0.682346, batch nll 0.382494, batch error rate 16.000000%\n",
      "At minibatch 13500, batch loss 0.570958, batch nll 0.269787, batch error rate 4.000000%\n",
      "At minibatch 13600, batch loss 0.682330, batch nll 0.379983, batch error rate 8.000000%\n",
      "At minibatch 13700, batch loss 0.582835, batch nll 0.279336, batch error rate 8.000000%\n",
      "At minibatch 13800, batch loss 0.598812, batch nll 0.294138, batch error rate 16.000000%\n",
      "At minibatch 13900, batch loss 0.944656, batch nll 0.639159, batch error rate 20.000000%\n",
      "At minibatch 14000, batch loss 0.428189, batch nll 0.121673, batch error rate 4.000000%\n",
      "At minibatch 14100, batch loss 0.723762, batch nll 0.415971, batch error rate 12.000000%\n",
      "At minibatch 14200, batch loss 0.820271, batch nll 0.511239, batch error rate 28.000000%\n",
      "At minibatch 14300, batch loss 0.952736, batch nll 0.642713, batch error rate 20.000000%\n",
      "At minibatch 14400, batch loss 1.453220, batch nll 1.142529, batch error rate 28.000000%\n",
      "After epoch 9: valid_err_rate: 28.430000% currently going to do 28 epochs\n",
      "After epoch 9: averaged train_err_rate: 15.050000% averaged train nll: 0.429958 averaged train loss: 0.732440\n",
      "At minibatch 14500, batch loss 0.527345, batch nll 0.216155, batch error rate 8.000000%\n",
      "At minibatch 14600, batch loss 0.676370, batch nll 0.364863, batch error rate 16.000000%\n",
      "At minibatch 14700, batch loss 0.725192, batch nll 0.413034, batch error rate 8.000000%\n",
      "At minibatch 14800, batch loss 0.471093, batch nll 0.158634, batch error rate 4.000000%\n",
      "At minibatch 14900, batch loss 0.716506, batch nll 0.402931, batch error rate 16.000000%\n",
      "At minibatch 15000, batch loss 0.680778, batch nll 0.365996, batch error rate 8.000000%\n",
      "At minibatch 15100, batch loss 0.733828, batch nll 0.417804, batch error rate 12.000000%\n",
      "At minibatch 15200, batch loss 0.730993, batch nll 0.414057, batch error rate 24.000000%\n",
      "At minibatch 15300, batch loss 1.368134, batch nll 1.050046, batch error rate 20.000000%\n",
      "At minibatch 15400, batch loss 0.814403, batch nll 0.495119, batch error rate 20.000000%\n",
      "At minibatch 15500, batch loss 0.981320, batch nll 0.660836, batch error rate 20.000000%\n",
      "At minibatch 15600, batch loss 0.768379, batch nll 0.446808, batch error rate 20.000000%\n",
      "At minibatch 15700, batch loss 0.778370, batch nll 0.455857, batch error rate 16.000000%\n",
      "At minibatch 15800, batch loss 0.913008, batch nll 0.589600, batch error rate 20.000000%\n",
      "At minibatch 15900, batch loss 0.860218, batch nll 0.535459, batch error rate 16.000000%\n",
      "At minibatch 16000, batch loss 0.584401, batch nll 0.258604, batch error rate 8.000000%\n",
      "After epoch 10: valid_err_rate: 30.210000% currently going to do 28 epochs\n",
      "After epoch 10: averaged train_err_rate: 13.987500% averaged train nll: 0.397612 averaged train loss: 0.714919\n",
      "At minibatch 16100, batch loss 0.648892, batch nll 0.322746, batch error rate 12.000000%\n",
      "At minibatch 16200, batch loss 0.594227, batch nll 0.268170, batch error rate 8.000000%\n",
      "At minibatch 16300, batch loss 0.725462, batch nll 0.399486, batch error rate 16.000000%\n",
      "At minibatch 16400, batch loss 0.683875, batch nll 0.357982, batch error rate 16.000000%\n",
      "At minibatch 16500, batch loss 0.532768, batch nll 0.206956, batch error rate 8.000000%\n",
      "At minibatch 16600, batch loss 0.541073, batch nll 0.215339, batch error rate 8.000000%\n",
      "At minibatch 16700, batch loss 0.521951, batch nll 0.196286, batch error rate 8.000000%\n",
      "At minibatch 16800, batch loss 0.637634, batch nll 0.312032, batch error rate 8.000000%\n",
      "At minibatch 16900, batch loss 0.516065, batch nll 0.190532, batch error rate 4.000000%\n",
      "At minibatch 17000, batch loss 0.471898, batch nll 0.146437, batch error rate 4.000000%\n",
      "At minibatch 17100, batch loss 0.816002, batch nll 0.490607, batch error rate 12.000000%\n",
      "At minibatch 17200, batch loss 0.444935, batch nll 0.119605, batch error rate 4.000000%\n",
      "At minibatch 17300, batch loss 0.495537, batch nll 0.170274, batch error rate 8.000000%\n",
      "At minibatch 17400, batch loss 0.629812, batch nll 0.304617, batch error rate 8.000000%\n",
      "At minibatch 17500, batch loss 0.568186, batch nll 0.243062, batch error rate 8.000000%\n",
      "At minibatch 17600, batch loss 0.565623, batch nll 0.240566, batch error rate 4.000000%\n",
      "After epoch 11: valid_err_rate: 25.870000% currently going to do 34 epochs\n",
      "After epoch 11: averaged train_err_rate: 9.092500% averaged train nll: 0.275757 averaged train loss: 0.601368\n",
      "At minibatch 17700, batch loss 0.670752, batch nll 0.345756, batch error rate 8.000000%\n",
      "At minibatch 17800, batch loss 0.548556, batch nll 0.223624, batch error rate 8.000000%\n",
      "At minibatch 17900, batch loss 0.495564, batch nll 0.170699, batch error rate 8.000000%\n",
      "At minibatch 18000, batch loss 0.555750, batch nll 0.230954, batch error rate 4.000000%\n",
      "At minibatch 18100, batch loss 0.621827, batch nll 0.297092, batch error rate 12.000000%\n",
      "At minibatch 18200, batch loss 0.562760, batch nll 0.238089, batch error rate 4.000000%\n",
      "At minibatch 18300, batch loss 0.396375, batch nll 0.071775, batch error rate 0.000000%\n",
      "At minibatch 18400, batch loss 0.467573, batch nll 0.143044, batch error rate 4.000000%\n",
      "At minibatch 18500, batch loss 0.403028, batch nll 0.078568, batch error rate 0.000000%\n",
      "At minibatch 18600, batch loss 0.459922, batch nll 0.135529, batch error rate 0.000000%\n",
      "At minibatch 18700, batch loss 0.429279, batch nll 0.104960, batch error rate 0.000000%\n",
      "At minibatch 18800, batch loss 0.443294, batch nll 0.119043, batch error rate 0.000000%\n",
      "At minibatch 18900, batch loss 0.494357, batch nll 0.170176, batch error rate 0.000000%\n",
      "At minibatch 19000, batch loss 0.500263, batch nll 0.176149, batch error rate 4.000000%\n",
      "At minibatch 19100, batch loss 0.421734, batch nll 0.097691, batch error rate 4.000000%\n",
      "At minibatch 19200, batch loss 0.399607, batch nll 0.075634, batch error rate 4.000000%\n",
      "After epoch 12: valid_err_rate: 25.400000% currently going to do 37 epochs\n",
      "After epoch 12: averaged train_err_rate: 6.177500% averaged train nll: 0.204947 averaged train loss: 0.529472\n",
      "At minibatch 19300, batch loss 0.442052, batch nll 0.118147, batch error rate 0.000000%\n",
      "At minibatch 19400, batch loss 0.727732, batch nll 0.403899, batch error rate 12.000000%\n",
      "At minibatch 19500, batch loss 0.670718, batch nll 0.346954, batch error rate 8.000000%\n",
      "At minibatch 19600, batch loss 0.579752, batch nll 0.256055, batch error rate 4.000000%\n",
      "At minibatch 19700, batch loss 0.562673, batch nll 0.239042, batch error rate 8.000000%\n",
      "At minibatch 19800, batch loss 0.472174, batch nll 0.148607, batch error rate 4.000000%\n",
      "At minibatch 19900, batch loss 0.743738, batch nll 0.420244, batch error rate 16.000000%\n",
      "At minibatch 20000, batch loss 0.409336, batch nll 0.085909, batch error rate 4.000000%\n",
      "At minibatch 20100, batch loss 0.450324, batch nll 0.126963, batch error rate 4.000000%\n",
      "At minibatch 20200, batch loss 0.560018, batch nll 0.236722, batch error rate 12.000000%\n",
      "At minibatch 20300, batch loss 0.618472, batch nll 0.295245, batch error rate 8.000000%\n",
      "At minibatch 20400, batch loss 0.514089, batch nll 0.190928, batch error rate 8.000000%\n",
      "At minibatch 20500, batch loss 0.616149, batch nll 0.293051, batch error rate 16.000000%\n",
      "At minibatch 20600, batch loss 0.482608, batch nll 0.159582, batch error rate 4.000000%\n",
      "At minibatch 20700, batch loss 0.444122, batch nll 0.121164, batch error rate 4.000000%\n",
      "At minibatch 20800, batch loss 0.434821, batch nll 0.111933, batch error rate 0.000000%\n",
      "After epoch 13: valid_err_rate: 25.110000% currently going to do 40 epochs\n",
      "After epoch 13: averaged train_err_rate: 5.167500% averaged train nll: 0.181783 averaged train loss: 0.505212\n",
      "At minibatch 20900, batch loss 0.465809, batch nll 0.142984, batch error rate 0.000000%\n",
      "At minibatch 21000, batch loss 0.454832, batch nll 0.132078, batch error rate 4.000000%\n",
      "At minibatch 21100, batch loss 0.452623, batch nll 0.129937, batch error rate 4.000000%\n",
      "At minibatch 21200, batch loss 0.412563, batch nll 0.089948, batch error rate 0.000000%\n",
      "At minibatch 21300, batch loss 0.408852, batch nll 0.086301, batch error rate 0.000000%\n",
      "At minibatch 21400, batch loss 0.498940, batch nll 0.176459, batch error rate 8.000000%\n",
      "At minibatch 21500, batch loss 0.601634, batch nll 0.279224, batch error rate 4.000000%\n",
      "At minibatch 21600, batch loss 0.526302, batch nll 0.203960, batch error rate 4.000000%\n",
      "At minibatch 21700, batch loss 0.545315, batch nll 0.223045, batch error rate 8.000000%\n",
      "At minibatch 21800, batch loss 0.519866, batch nll 0.197664, batch error rate 4.000000%\n",
      "At minibatch 21900, batch loss 0.460619, batch nll 0.138485, batch error rate 4.000000%\n",
      "At minibatch 22000, batch loss 0.527002, batch nll 0.204937, batch error rate 4.000000%\n",
      "At minibatch 22100, batch loss 0.528325, batch nll 0.206329, batch error rate 4.000000%\n",
      "At minibatch 22200, batch loss 0.496424, batch nll 0.174493, batch error rate 4.000000%\n",
      "At minibatch 22300, batch loss 0.362051, batch nll 0.040188, batch error rate 0.000000%\n",
      "At minibatch 22400, batch loss 0.443498, batch nll 0.121705, batch error rate 4.000000%\n",
      "After epoch 14: valid_err_rate: 24.800000% currently going to do 43 epochs\n",
      "After epoch 14: averaged train_err_rate: 4.512500% averaged train nll: 0.166961 averaged train loss: 0.489303\n",
      "At minibatch 22500, batch loss 0.542362, batch nll 0.220639, batch error rate 12.000000%\n",
      "At minibatch 22600, batch loss 0.458805, batch nll 0.137148, batch error rate 4.000000%\n",
      "At minibatch 22700, batch loss 0.466538, batch nll 0.144945, batch error rate 8.000000%\n",
      "At minibatch 22800, batch loss 0.387912, batch nll 0.066385, batch error rate 0.000000%\n",
      "At minibatch 22900, batch loss 0.462066, batch nll 0.140608, batch error rate 4.000000%\n",
      "At minibatch 23000, batch loss 0.602325, batch nll 0.280936, batch error rate 12.000000%\n",
      "At minibatch 23100, batch loss 0.435341, batch nll 0.114019, batch error rate 4.000000%\n",
      "At minibatch 23200, batch loss 0.408294, batch nll 0.087041, batch error rate 0.000000%\n",
      "At minibatch 23300, batch loss 0.442304, batch nll 0.121125, batch error rate 0.000000%\n",
      "At minibatch 23400, batch loss 0.424532, batch nll 0.103418, batch error rate 0.000000%\n",
      "At minibatch 23500, batch loss 0.481553, batch nll 0.160506, batch error rate 4.000000%\n",
      "At minibatch 23600, batch loss 0.394658, batch nll 0.073681, batch error rate 4.000000%\n",
      "At minibatch 23700, batch loss 0.437222, batch nll 0.116313, batch error rate 0.000000%\n",
      "At minibatch 23800, batch loss 0.553959, batch nll 0.233119, batch error rate 4.000000%\n",
      "At minibatch 23900, batch loss 0.473624, batch nll 0.152854, batch error rate 4.000000%\n",
      "At minibatch 24000, batch loss 0.392264, batch nll 0.071566, batch error rate 0.000000%\n",
      "After epoch 15: valid_err_rate: 24.700000% currently going to do 46 epochs\n",
      "After epoch 15: averaged train_err_rate: 4.035000% averaged train nll: 0.154544 averaged train loss: 0.475794\n",
      "At minibatch 24100, batch loss 0.445363, batch nll 0.124734, batch error rate 4.000000%\n",
      "At minibatch 24200, batch loss 0.514818, batch nll 0.194257, batch error rate 4.000000%\n",
      "At minibatch 24300, batch loss 0.457469, batch nll 0.136976, batch error rate 4.000000%\n",
      "At minibatch 24400, batch loss 0.457149, batch nll 0.136717, batch error rate 4.000000%\n",
      "At minibatch 24500, batch loss 0.524771, batch nll 0.204409, batch error rate 8.000000%\n",
      "At minibatch 24600, batch loss 0.487667, batch nll 0.167377, batch error rate 4.000000%\n",
      "At minibatch 24700, batch loss 0.620324, batch nll 0.300104, batch error rate 4.000000%\n",
      "At minibatch 24800, batch loss 0.417287, batch nll 0.097132, batch error rate 0.000000%\n",
      "At minibatch 24900, batch loss 0.432152, batch nll 0.112066, batch error rate 4.000000%\n",
      "At minibatch 25000, batch loss 0.448801, batch nll 0.128784, batch error rate 4.000000%\n",
      "At minibatch 25100, batch loss 0.512074, batch nll 0.192129, batch error rate 8.000000%\n",
      "At minibatch 25200, batch loss 0.407883, batch nll 0.088007, batch error rate 0.000000%\n",
      "At minibatch 25300, batch loss 0.499209, batch nll 0.179400, batch error rate 4.000000%\n",
      "At minibatch 25400, batch loss 0.435694, batch nll 0.115955, batch error rate 0.000000%\n",
      "At minibatch 25500, batch loss 0.621908, batch nll 0.302240, batch error rate 8.000000%\n",
      "At minibatch 25600, batch loss 0.387481, batch nll 0.067879, batch error rate 0.000000%\n",
      "After epoch 16: valid_err_rate: 24.830000% currently going to do 46 epochs\n",
      "After epoch 16: averaged train_err_rate: 3.705000% averaged train nll: 0.146041 averaged train loss: 0.466193\n",
      "At minibatch 25700, batch loss 0.436609, batch nll 0.117078, batch error rate 4.000000%\n",
      "At minibatch 25800, batch loss 0.493361, batch nll 0.173899, batch error rate 4.000000%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3c44e1f8651d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwdec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#print [p.get_value().ravel()[:10] for p in model_parameters]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/pio/os/anaconda/lib/python2.7/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "while e<number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in cifar_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        #print i\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 #* K / np.maximum(K, i)\n",
    "        #try 1e-3, 1e-4 after e\n",
    "        if e>10:\n",
    "            lrate=1e-4\n",
    "        if e>20:\n",
    "            lrate=1e-5\n",
    "        if e>40:\n",
    "            lrate=1e-6\n",
    "        momentum=0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch,1, lrate, momentum)\n",
    "        \n",
    "        #print [p.get_value().ravel()[:10] for p in model_parameters]\n",
    "        #print [p.get_value().ravel()[:10] for p in velocities]\n",
    "        \n",
    "        \n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs)\n",
    "\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 15\n",
      "Test error rate is 24.930000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3f913bd7d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3fd29c6150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" %(compute_error_rate(cifar_test_stream)*100.0,)\n",
    "\n",
    "subplot(2,1,1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:,0], train_nll_a[:,1], label='batch train nll')\n",
    "legend()\n",
    "\n",
    "subplot(2,1,2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "\n",
    "plot(train_erros_a[:,0], train_erros_a[:,1], label='batch train error rate')\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:,0], validation_errors_a[:,1], label='validation error rate', color='r')\n",
    "ylim(0.15,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr_errs = 0.0\n",
    "pr_num_samples = 0.0\n",
    "arr_ans=[]\n",
    "arr_corr=[]\n",
    "\n",
    "for X, Y in cifar_test_stream.get_epoch_iterator():\n",
    "    pr_errs += (predict(X,0)!=Y.ravel()).sum()\n",
    "    pr_num_samples += Y.shape[0]\n",
    "    #print predict(X,0),Y.ravel()\n",
    "    arr_ans.extend(predict(X,0))\n",
    "    arr_corr.extend(Y.ravel())\n",
    "\n",
    "#print pr_errs,pr_num_samples,arr_ans,arr_corr\n",
    "f = open('results','w')\n",
    "for x,y in zip(arr_ans,arr_corr):\n",
    "    f.write('%d %d\\n'% (x,y))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
